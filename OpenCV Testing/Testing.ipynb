{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Person Detection and Movement Tracking System\n",
    "\n",
    "This system uses OpenCV to detect people via webcam and tracks their movements to identify:\n",
    "- People walking back and forth\n",
    "- Anyone approaching your computer\n",
    "- Real-time alerts for suspicious activity\n"
   ],
   "id": "de7db694f6833c07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:53:36.040118Z",
     "start_time": "2025-07-18T02:53:35.586594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pygame\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize pygame for sound alerts\n",
    "pygame.mixer.init()\n",
    "\n",
    "class MultiModelPersonDetector:\n",
    "    def __init__(self):\n",
    "        # Available detection models\n",
    "        self.models = {\n",
    "            'hog': 'HOG + SVM (OpenCV)',\n",
    "            'yolo': 'YOLOv8 (Ultralytics)',\n",
    "            'mobilenet': 'MobileNet SSD (OpenCV DNN)',\n",
    "            'cascade': 'Haar Cascade (OpenCV)',\n",
    "            'background_subtraction': 'Background Subtraction + Contours'\n",
    "        }\n",
    "\n",
    "        self.current_model = 'yolo'\n",
    "        self.model_objects = {}\n",
    "\n",
    "        # Initialize all models\n",
    "        self._initialize_models()\n",
    "\n",
    "        # Movement tracking\n",
    "        self.person_tracks = {}\n",
    "        self.track_id = 0\n",
    "        self.approach_threshold = 0.3\n",
    "        self.oscillation_threshold = 3\n",
    "        self.pacing_alert_window = 10  # seconds\n",
    "        self.pacing_tracks = {}  # track_id: {'start_time': float, 'last_seen': float, 'direction_changes': int}\n",
    "\n",
    "        # Alert settings\n",
    "        self.last_alert_time = 0\n",
    "        self.alert_cooldown = 5\n",
    "\n",
    "        # Statistics\n",
    "        self.detection_count = 0\n",
    "        self.approach_alerts = 0\n",
    "        self.pacing_alerts = 0\n",
    "\n",
    "        # Performance tracking\n",
    "        self.fps_history = deque(maxlen=30)\n",
    "        self.detection_confidence = deque(maxlen=30)\n",
    "\n",
    "        # Notification settings\n",
    "        self.notification_message = None\n",
    "        self.notification_start_time = 0\n",
    "        self.notification_duration = 3  # seconds\n",
    "        self.notification_flash = False\n",
    "        self.notification_flash_interval = 0.3  # seconds\n",
    "        self.last_flash_time = 0\n",
    "\n",
    "        # Close distance alert settings\n",
    "        self.close_alert_distance = 0.45\n",
    "        self.close_alert_time = 2  # seconds\n",
    "        self.close_tracks = {}  # track_id: {'start_time': float, 'last_seen': float, 'alerted': bool}\n",
    "\n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Initialize all detection models\"\"\"\n",
    "        print(\"Initializing detection models...\")\n",
    "\n",
    "        # 1. HOG + SVM\n",
    "        try:\n",
    "            hog = cv2.HOGDescriptor()\n",
    "            hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "            self.model_objects['hog'] = hog\n",
    "            print(\"✓ HOG + SVM initialized\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ HOG + SVM failed: {e}\")\n",
    "\n",
    "        # 2. YOLOv8\n",
    "        try:\n",
    "            from ultralytics import YOLO\n",
    "            import torch\n",
    "            device = 'mps' if hasattr(torch, 'backends') and torch.backends.mps.is_available() else 'cpu'\n",
    "            yolo_model = YOLO('yolov8n.pt')\n",
    "            yolo_model.to(device)\n",
    "            self.model_objects['yolo'] = yolo_model\n",
    "            print(f\"✓ YOLOv8 initialized on device: {device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ YOLOv8 failed: {e}\")\n",
    "\n",
    "        # 3. MobileNet SSD\n",
    "        try:\n",
    "            # Download MobileNet SSD model if not exists\n",
    "            model_dir = Path(\"models\")\n",
    "            model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            config_path = model_dir / \"MobileNetSSD_deploy.prototxt\"\n",
    "            weights_path = model_dir / \"MobileNetSSD_deploy.caffemodel\"\n",
    "\n",
    "            # For demo purposes, we'll use a simpler approach\n",
    "            # You can manually download these files if needed\n",
    "            try:\n",
    "                net = cv2.dnn.readNetFromCaffe(str(config_path), str(weights_path))\n",
    "                self.model_objects['mobilenet'] = net\n",
    "                print(\"✓ MobileNet SSD initialized\")\n",
    "            except:\n",
    "                print(\"✗ MobileNet SSD files not found (download required)\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ MobileNet SSD failed: {e}\")\n",
    "\n",
    "        # 4. Haar Cascade\n",
    "        try:\n",
    "            # Use correct path to cascade file\n",
    "            cascade_path = os.path.join(cv2.data.haarcascades, 'haarcascade_fullbody.xml')\n",
    "            if os.path.exists(cascade_path):\n",
    "                cascade = cv2.CascadeClassifier(cascade_path)\n",
    "                self.model_objects['cascade'] = cascade\n",
    "                print(\"✓ Haar Cascade initialized\")\n",
    "            else:\n",
    "                print(\"✗ Haar Cascade file not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Haar Cascade failed: {e}\")\n",
    "\n",
    "        # 5. Background Subtraction\n",
    "        try:\n",
    "            bg_subtractor = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n",
    "            self.model_objects['background_subtraction'] = bg_subtractor\n",
    "            print(\"✓ Background Subtraction initialized\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Background Subtraction failed: {e}\")\n",
    "\n",
    "    def switch_model(self, model_name):\n",
    "        \"\"\"Switch to a different detection model\"\"\"\n",
    "        if model_name in self.models and model_name in self.model_objects:\n",
    "            self.current_model = model_name\n",
    "            print(f\"Switched to: {self.models[model_name]}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Model '{model_name}' not available\")\n",
    "            return False\n",
    "\n",
    "    def detect_people_hog(self, frame):\n",
    "        \"\"\"HOG + SVM detection\"\"\"\n",
    "        if 'hog' not in self.model_objects:\n",
    "            return [], []\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        boxes, weights = self.model_objects['hog'].detectMultiScale(\n",
    "            gray,\n",
    "            winStride=(8, 8),\n",
    "            padding=(32, 32),\n",
    "            scale=1.05\n",
    "        )\n",
    "\n",
    "        # Convert to [x1, y1, x2, y2] format\n",
    "        boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "        return boxes, weights\n",
    "\n",
    "    def detect_people_yolo(self, frame):\n",
    "        \"\"\"YOLOv8 detection\"\"\"\n",
    "        if 'yolo' not in self.model_objects:\n",
    "            return [], []\n",
    "\n",
    "        results = self.model_objects['yolo'](frame, verbose=False)\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                if box.cls == 0:  # Person class\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                    boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "                    confidences.append(float(box.conf))\n",
    "\n",
    "        return np.array(boxes), np.array(confidences)\n",
    "\n",
    "    def detect_people_mobilenet(self, frame):\n",
    "        \"\"\"MobileNet SSD detection\"\"\"\n",
    "        if 'mobilenet' not in self.model_objects:\n",
    "            return [], []\n",
    "\n",
    "        h, w = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), (127.5, 127.5, 127.5))\n",
    "\n",
    "        net = self.model_objects['mobilenet']\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            class_id = int(detections[0, 0, i, 1])\n",
    "\n",
    "            if class_id == 15 and confidence > 0.5:  # Person class\n",
    "                x1 = int(detections[0, 0, i, 3] * w)\n",
    "                y1 = int(detections[0, 0, i, 4] * h)\n",
    "                x2 = int(detections[0, 0, i, 5] * w)\n",
    "                y2 = int(detections[0, 0, i, 6] * h)\n",
    "\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                confidences.append(confidence)\n",
    "\n",
    "        return np.array(boxes), np.array(confidences)\n",
    "\n",
    "    def detect_people_cascade(self, frame):\n",
    "        \"\"\"Haar Cascade detection\"\"\"\n",
    "        if 'cascade' not in self.model_objects:\n",
    "            return [], []\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        bodies = self.model_objects['cascade'].detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=3,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "\n",
    "        # Convert to [x1, y1, x2, y2] format\n",
    "        boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in bodies])\n",
    "        weights = np.ones(len(boxes))  # No confidence scores from cascade\n",
    "\n",
    "        return boxes, weights\n",
    "\n",
    "    def detect_people_background_subtraction(self, frame):\n",
    "        \"\"\"Background subtraction with contour detection\"\"\"\n",
    "        if 'background_subtraction' not in self.model_objects:\n",
    "            return [], []\n",
    "\n",
    "        # Apply background subtraction\n",
    "        fg_mask = self.model_objects['background_subtraction'].apply(frame)\n",
    "\n",
    "        # Morphological operations to clean up the mask\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 500:  # Filter small contours\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                # Filter by aspect ratio (rough person shape)\n",
    "                aspect_ratio = h / w\n",
    "                if 1.2 < aspect_ratio < 4.0:\n",
    "                    boxes.append([x, y, x + w, y + h])\n",
    "                    confidences.append(area / 10000)  # Use area as confidence\n",
    "\n",
    "        return np.array(boxes), np.array(confidences)\n",
    "\n",
    "    def detect_people(self, frame):\n",
    "        \"\"\"Detect people using the current model\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if self.current_model == 'hog':\n",
    "            boxes, weights = self.detect_people_hog(frame)\n",
    "        elif self.current_model == 'yolo':\n",
    "            boxes, weights = self.detect_people_yolo(frame)\n",
    "        elif self.current_model == 'mobilenet':\n",
    "            boxes, weights = self.detect_people_mobilenet(frame)\n",
    "        elif self.current_model == 'cascade':\n",
    "            boxes, weights = self.detect_people_cascade(frame)\n",
    "        elif self.current_model == 'background_subtraction':\n",
    "            boxes, weights = self.detect_people_background_subtraction(frame)\n",
    "        else:\n",
    "            boxes, weights = [], []\n",
    "\n",
    "        # Calculate FPS\n",
    "        detection_time = time.time() - start_time\n",
    "        fps = 1.0 / detection_time if detection_time > 0 else 0\n",
    "        self.fps_history.append(fps)\n",
    "\n",
    "        return boxes, weights\n",
    "\n",
    "    def track_movement(self, boxes, frame_center):\n",
    "        \"\"\"Track person movement and detect patterns\"\"\"\n",
    "        current_time = time.time()\n",
    "        frame_height, frame_width = frame_center\n",
    "\n",
    "        alerts = []\n",
    "        active_track_ids = set()\n",
    "\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            center_x = (x1 + x2) // 2\n",
    "            center_y = (y1 + y2) // 2\n",
    "\n",
    "            # Find closest existing track or create new one\n",
    "            min_distance = float('inf')\n",
    "            closest_track = None\n",
    "\n",
    "            for track_id, track_data in self.person_tracks.items():\n",
    "                if track_data['positions']:\n",
    "                    last_pos = track_data['positions'][-1]\n",
    "                    dist = np.sqrt((center_x - last_pos[0])**2 + (center_y - last_pos[1])**2)\n",
    "                    if dist < min_distance and dist < 100:  # Maximum movement threshold\n",
    "                        min_distance = dist\n",
    "                        closest_track = track_id\n",
    "\n",
    "            if closest_track is None:\n",
    "                # Create new track\n",
    "                self.track_id += 1\n",
    "                self.person_tracks[self.track_id] = {\n",
    "                    'positions': deque(maxlen=30),  # Keep last 30 positions\n",
    "                    'timestamps': deque(maxlen=30),\n",
    "                    'direction_changes': 0,\n",
    "                    'last_direction': None,\n",
    "                }\n",
    "                closest_track = self.track_id\n",
    "\n",
    "            # Update track\n",
    "            track = self.person_tracks[closest_track]\n",
    "            track['positions'].append((center_x, center_y))\n",
    "            track['timestamps'].append(current_time)\n",
    "\n",
    "            # Pacing logic\n",
    "            if closest_track not in self.pacing_tracks:\n",
    "                self.pacing_tracks[closest_track] = {\n",
    "                    'start_time': current_time,\n",
    "                    'last_seen': current_time,\n",
    "                    'direction_changes': 0,\n",
    "                    'last_direction': None,\n",
    "                    'alerted': False\n",
    "                }\n",
    "            pacing_track = self.pacing_tracks[closest_track]\n",
    "            pacing_track['last_seen'] = current_time\n",
    "\n",
    "            # Detect pacing/oscillation\n",
    "            if len(track['positions']) >= 3:\n",
    "                positions = list(track['positions'])\n",
    "                recent_direction = \"left\" if positions[-1][0] < positions[-3][0] else \"right\"\n",
    "\n",
    "                if pacing_track['last_direction'] and pacing_track['last_direction'] != recent_direction:\n",
    "                    pacing_track['direction_changes'] += 1\n",
    "\n",
    "                pacing_track['last_direction'] = recent_direction\n",
    "\n",
    "            active_track_ids.add(closest_track)\n",
    "\n",
    "            # --- New: Close distance alert logic ---\n",
    "            # Use the distance calculation from run_distance_demo\n",
    "            distance = self.calculate_distance_to_camera(box)\n",
    "            if closest_track not in self.close_tracks:\n",
    "                self.close_tracks[closest_track] = {\n",
    "                    'start_time': None,\n",
    "                    'last_seen': current_time,\n",
    "                    'alerted': False\n",
    "                }\n",
    "            close_track = self.close_tracks[closest_track]\n",
    "            close_track['last_seen'] = current_time\n",
    "            if distance < self.close_alert_distance:\n",
    "                if close_track['start_time'] is None:\n",
    "                    close_track['start_time'] = current_time\n",
    "                elif not close_track['alerted'] and (current_time - close_track['start_time'] >= self.close_alert_time):\n",
    "                    alerts.append(f\"Person {closest_track} is too close to the camera! (Dist: {distance:.2f})\")\n",
    "                    close_track['alerted'] = True\n",
    "            else:\n",
    "                close_track['start_time'] = None\n",
    "                close_track['alerted'] = False\n",
    "\n",
    "        # Clean up old tracks and handle pacing window\n",
    "        tracks_to_remove = []\n",
    "        for track_id, pacing_track in self.pacing_tracks.items():\n",
    "            if track_id not in active_track_ids:\n",
    "                # If not seen for >1s, keep pacing window open\n",
    "                if current_time - pacing_track['last_seen'] > 1:\n",
    "                    continue  # Don't remove yet\n",
    "            # If pacing for at least 10s and enough direction changes, alert\n",
    "            if not pacing_track['alerted'] and current_time - pacing_track['start_time'] >= self.pacing_alert_window and pacing_track['direction_changes'] >= self.oscillation_threshold:\n",
    "                alerts.append(f\"Person {track_id} is pacing back and forth!\")\n",
    "                pacing_track['alerted'] = True\n",
    "            # Remove tracks not seen for a long time\n",
    "            if current_time - pacing_track['last_seen'] > 5:\n",
    "                tracks_to_remove.append(track_id)\n",
    "\n",
    "        for track_id, close_track in self.close_tracks.items():\n",
    "            if track_id not in active_track_ids and current_time - close_track['last_seen'] > 5:\n",
    "                tracks_to_remove.append(track_id)\n",
    "\n",
    "        for track_id in tracks_to_remove:\n",
    "            if track_id in self.pacing_tracks:\n",
    "                del self.pacing_tracks[track_id]\n",
    "            if track_id in self.person_tracks:\n",
    "                del self.person_tracks[track_id]\n",
    "            if track_id in self.close_tracks:\n",
    "                del self.close_tracks[track_id]\n",
    "\n",
    "        return alerts\n",
    "\n",
    "    def play_alert_sound(self):\n",
    "        \"\"\"Play alert sound (beep)\"\"\"\n",
    "        try:\n",
    "            # Create a simple beep sound\n",
    "            sample_rate = 22050\n",
    "            duration = 0.5\n",
    "            frequency = 800\n",
    "\n",
    "            frames = int(duration * sample_rate)\n",
    "            arr = np.sin(2 * np.pi * frequency * np.linspace(0, duration, frames))\n",
    "            arr = (arr * 32767).astype(np.int16)\n",
    "\n",
    "            sound = pygame.sndarray.make_sound(arr)\n",
    "            sound.play()\n",
    "        except:\n",
    "            print(\"ALERT!\")  # Fallback if sound fails\n",
    "\n",
    "    def trigger_notification(self, message):\n",
    "        \"\"\"Trigger an on-screen notification\"\"\"\n",
    "        self.notification_message = message\n",
    "        self.notification_start_time = time.time()\n",
    "        self.notification_flash = True\n",
    "        self.last_flash_time = time.time()\n",
    "\n",
    "    def draw_detections(self, frame, boxes, alerts):\n",
    "        \"\"\"Draw bounding boxes and information on frame\"\"\"\n",
    "        for i, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = box\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            # Show distance above each box\n",
    "            distance = self.calculate_distance_to_camera(box)\n",
    "            cv2.putText(frame, f\"Dist: {distance:.2f}\", (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Display current model\n",
    "        cv2.putText(frame, f\"Model: {self.models[self.current_model]}\",\n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "\n",
    "        # Display FPS\n",
    "        if self.fps_history:\n",
    "            avg_fps = np.mean(list(self.fps_history))\n",
    "            cv2.putText(frame, f\"FPS: {avg_fps:.1f}\",\n",
    "                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "\n",
    "        # Display alerts\n",
    "        for i, alert in enumerate(alerts):\n",
    "            cv2.putText(frame, alert, (10, 90 + i*30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Display statistics\n",
    "        stats = [\n",
    "            f\"People detected: {len(boxes)}\",\n",
    "            f\"Total detections: {self.detection_count}\",\n",
    "            f\"Approach alerts: {self.approach_alerts}\",\n",
    "            f\"Pacing alerts: {self.pacing_alerts}\"\n",
    "        ]\n",
    "        for i, stat in enumerate(stats):\n",
    "            cv2.putText(frame, stat, (10, frame.shape[0] - 120 + i*20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        # On-screen notification overlay (flashing)\n",
    "        if self.notification_message:\n",
    "            now = time.time()\n",
    "            elapsed = now - self.notification_start_time\n",
    "            if elapsed < self.notification_duration:\n",
    "                # Flashing effect\n",
    "                if now - self.last_flash_time > self.notification_flash_interval:\n",
    "                    self.notification_flash = not self.notification_flash\n",
    "                    self.last_flash_time = now\n",
    "                if self.notification_flash:\n",
    "                    overlay = frame.copy()\n",
    "                    h, w = frame.shape[:2]\n",
    "                    cv2.rectangle(overlay, (0, 0), (w, h), (0, 0, 255), -1)\n",
    "                    alpha = 0.4\n",
    "                    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "                    cv2.putText(frame, self.notification_message, (int(w*0.1), int(h*0.5)),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 6, cv2.LINE_AA)\n",
    "            else:\n",
    "                self.notification_message = None\n",
    "        return frame\n",
    "\n",
    "    def calculate_distance_to_camera(self, box):\n",
    "        \"\"\"Estimate distance from camera to detected person\"\"\"\n",
    "        # Assuming box is in [x1, y1, x2, y2] format\n",
    "        # Calculate the width of the detected person in the image\n",
    "        person_width_pixels = box[2] - box[0]\n",
    "\n",
    "        # Assuming a constant real-world width for a person (e.g., 0.5 meters)\n",
    "        REAL_PERSON_WIDTH = 0.5  # meters\n",
    "\n",
    "        # Focal length estimation (this should be calibrated for your camera)\n",
    "        FOCAL_LENGTH = 800  # pixels (example value, needs calibration)\n",
    "\n",
    "        # Distance calculation using the formula:\n",
    "        # distance = (REAL_PERSON_WIDTH * FOCAL_LENGTH) / person_width_pixels\n",
    "        distance = (REAL_PERSON_WIDTH * FOCAL_LENGTH) / person_width_pixels if person_width_pixels > 0 else 0\n",
    "\n",
    "        return distance\n"
   ],
   "id": "a7d1f5a12588bc4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:53:36.056548Z",
     "start_time": "2025-07-18T02:53:36.053502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_person_detection():\n",
    "    \"\"\"Main function to run the person detection system\"\"\"\n",
    "    detector = MultiModelPersonDetector()\n",
    "\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "\n",
    "    print(\"Person Detection System Started\")\n",
    "    print(\"Press 'q' to quit\")\n",
    "    print(\"Press 's' to save screenshot\")\n",
    "    print(\"Press 'r' to reset statistics\")\n",
    "\n",
    "    screenshot_count = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame\")\n",
    "                break\n",
    "\n",
    "            # Flip the camera feed horizontally\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Detect people\n",
    "            boxes, weights = detector.detect_people(frame)\n",
    "            detector.detection_count += len(boxes)\n",
    "\n",
    "            # Track movement and get alerts\n",
    "            frame_center = (frame.shape[0], frame.shape[1])\n",
    "            alerts = detector.track_movement(boxes, frame_center)\n",
    "\n",
    "            # Handle alerts\n",
    "            current_time = time.time()\n",
    "            if alerts and current_time - detector.last_alert_time > detector.alert_cooldown:\n",
    "                # detector.play_alert_sound()  # Sound disabled for now\n",
    "                detector.last_alert_time = current_time\n",
    "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] ALERTS: {', '.join(alerts)}\")\n",
    "                detector.trigger_notification(\"Suspicious behavior detected!\")\n",
    "\n",
    "            # Draw detections and information\n",
    "            frame = detector.draw_detections(frame, boxes, alerts)\n",
    "\n",
    "            # Display frame\n",
    "            cv2.imshow('Person Detection - Security Monitor', frame)\n",
    "\n",
    "            # Handle key presses\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('s'):\n",
    "                screenshot_count += 1\n",
    "                filename = f\"data/screenshot_{screenshot_count}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "                cv2.imwrite(filename, frame)\n",
    "                print(f\"Screenshot saved: {filename}\")\n",
    "            elif key == ord('r'):\n",
    "                detector.detection_count = 0\n",
    "                detector.approach_alerts = 0\n",
    "                detector.pacing_alerts = 0\n",
    "                detector.person_tracks.clear()\n",
    "                print(\"Statistics reset\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping detection system...\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Person detection system stopped\")\n"
   ],
   "id": "c17d7c09ae22a724",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:53:36.064514Z",
     "start_time": "2025-07-18T02:53:36.061874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model comparison tool\n",
    "def compare_models():\n",
    "    \"\"\"Compare performance of different detection models\"\"\"\n",
    "    detector = MultiModelPersonDetector()\n",
    "\n",
    "    # Test each model on a sample frame\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "\n",
    "    ret, test_frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not capture test frame\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name in detector.models.keys():\n",
    "        if model_name in detector.model_objects:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Switch to model and detect\n",
    "            detector.switch_model(model_name)\n",
    "            boxes, weights = detector.detect_people(test_frame)\n",
    "\n",
    "            detection_time = time.time() - start_time\n",
    "            fps = 1.0 / detection_time if detection_time > 0 else 0\n",
    "\n",
    "            results[model_name] = {\n",
    "                'detections': len(boxes),\n",
    "                'fps': fps,\n",
    "                'avg_confidence': np.mean(weights) if len(weights) > 0 else 0\n",
    "            }\n",
    "\n",
    "            print(f\"{detector.models[model_name]}:\")\n",
    "            print(f\"  Detections: {len(boxes)}\")\n",
    "            print(f\"  FPS: {fps:.1f}\")\n",
    "            print(f\"  Avg Confidence: {results[model_name]['avg_confidence']:.2f}\")\n",
    "            print()\n",
    "\n",
    "    # Recommend best model\n",
    "    if results:\n",
    "        best_fps = max(results.items(), key=lambda x: x[1]['fps'])\n",
    "        best_detections = max(results.items(), key=lambda x: x[1]['detections'])\n",
    "\n",
    "        print(\"Recommendations:\")\n",
    "        print(f\"  Best Speed: {detector.models[best_fps[0]]} ({best_fps[1]['fps']:.1f} FPS)\")\n",
    "        print(f\"  Most Detections: {detector.models[best_detections[0]]} ({best_detections[1]['detections']} people)\")\n",
    "\n",
    "# Uncomment to run model comparison\n",
    "# compare_models()\n"
   ],
   "id": "1f7037997732b44c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advanced Features\n",
    "\n",
    "You can extend this system with:\n",
    "- Face recognition to identify specific individuals\n",
    "- Motion history tracking\n",
    "- Email/SMS alerts for security breaches\n",
    "- Integration with home automation systems\n",
    "- Multiple camera support\n"
   ],
   "id": "596453e22762cc1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:53:36.070839Z",
     "start_time": "2025-07-18T02:53:36.068945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optional: Analyze detection patterns\n",
    "def analyze_detection_patterns():\n",
    "    \"\"\"Analyze patterns in the detection data\"\"\"\n",
    "    # This would analyze stored detection data\n",
    "    # For now, just show how to visualize detection frequency\n",
    "\n",
    "    # Example visualization\n",
    "    times = [datetime.now().hour + np.random.randint(-2, 3) for _ in range(50)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(times, bins=24, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Detection Frequency')\n",
    "    plt.title('Person Detection Frequency by Hour')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to run analysis\n",
    "# analyze_detection_patterns()\n"
   ],
   "id": "c2d722424a2578a7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Run the Detection System\n",
    "\n",
    "Execute the cell below to start the person detection system with interactive model selection.\n"
   ],
   "id": "d16954dce0a2c46a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:53:36.077507Z",
     "start_time": "2025-07-18T02:53:36.075501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_distance_demo():\n",
    "    detector = MultiModelPersonDetector()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    print(\"Distance Demo Started. Press 'q' to quit.\")\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame\")\n",
    "                break\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            boxes, _ = detector.detect_people(frame)\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                # Calculate and display distance\n",
    "                distance = detector.calculate_distance_to_camera(box)\n",
    "                cv2.putText(frame, f\"Dist: {distance:.2f}\", (x1, y1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.imshow('Distance Demo', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Distance demo stopped.\")\n"
   ],
   "id": "5c77c82126546b78",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:53:47.482942Z",
     "start_time": "2025-07-18T02:53:36.082017Z"
    }
   },
   "cell_type": "code",
   "source": "run_person_detection()",
   "id": "355b1d4a20e27c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing detection models...\n",
      "✓ HOG + SVM initialized\n",
      "✓ YOLOv8 initialized on device: mps\n",
      "✗ MobileNet SSD files not found (download required)\n",
      "✓ Haar Cascade initialized\n",
      "✓ Background Subtraction initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 22:53:36.812 python[70554:33931437] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person Detection System Started\n",
      "Press 'q' to quit\n",
      "Press 's' to save screenshot\n",
      "Press 'r' to reset statistics\n",
      "[22:53:43] ALERTS: Person 1 is too close to the camera! (Dist: 0.39)\n",
      "Person detection system stopped\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:53:47.492541Z",
     "start_time": "2025-07-18T02:53:47.491337Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "71fc7ff4f7193287",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
